import sys
import os

# Adiciona o diret√≥rio raiz do projeto ao Python path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from config.settings import Config
from src.api.intercom_client import IntercomClient
from src.processing.text_processor import TextProcessor
from src.processing.chunker import LLMChunker
from src.processing.contextual_enricher import ContextualEnricher
from src.processing.categorizer import ArticleCategorizer
from src.utils.embeddings import EmbeddingGenerator
from src.utils.text_cleaner import TextCleaner  # ‚úÖ Novo m√≥dulo unificado
# ‚úÖ NOVA CONFIGURA√á√ÉO: IDs que devem ter todos os idiomas (PT, EN, ES)
MULTILINGUAL_ARTICLE_IDS = [
    "7861149", "7915496", "8411647", "8887223", "7915619",
    "7861109", "10008263", "7885145", "7992438", "7914908"
]

# Fun√ß√£o igual ao pipeline principal
def get_allowed_languages(article_id: str, multilingual_article_ids: list) -> list:
    """
    Determina quais idiomas processar baseado no ID do artigo.
    - Para IDs espec√≠ficos: processa PT, EN, ES
    - Para demais artigos: apenas PT-BR
    """
    if str(article_id) in multilingual_article_ids:
        return ["pt", "pt-BR", "en", "es"]  # Todos os idiomas para artigos espec√≠ficos
    else:
        return ["pt", "pt-BR"]  # Aceita pt e pt-BR para os demais


def is_rag_eligible_article(article: dict, rag_collection_id: str = None, excluded_article_ids: list = None) -> bool:
    """Determina se um artigo √© eleg√≠vel para o RAG baseado na cole√ß√£o e lista de exclus√µes."""
    article_id = str(article.get("id", ""))
    if excluded_article_ids and article_id in excluded_article_ids:
        return False
    if rag_collection_id:
        parent_ids = article.get("parent_ids", [])
        if rag_collection_id not in [str(pid) for pid in parent_ids]:
            return False

    translated_content = article.get("translated_content", {})
    if not translated_content:
        return False

    for _, content in translated_content.items():
        if isinstance(content, dict) and content.get("body"):
            state = content.get("state", "")
            if state == "published" or (state == "draft" and rag_collection_id):
                return True
    return False


def process_single_article_test(article: dict, components: dict, rag_collection_id: str = None, excluded_article_ids: list = None) -> list:
    """
    Vers√£o de teste que processa um artigo seguindo as melhores pr√°ticas
    mas N√ÉO salva no MongoDB. Mostra todo o pipeline em a√ß√£o.
    """
    article_id = article.get("id")
    documents_for_db = []

    if not is_rag_eligible_article(article, rag_collection_id, excluded_article_ids):
        if excluded_article_ids and str(article_id) in excluded_article_ids:
            print(f" -> Artigo {article_id} pulado: est√° na lista de exclus√µes.")
        else:
            print(f" -> Artigo {article_id} pulado: n√£o est√° na cole√ß√£o RAG ou n√£o tem conte√∫do v√°lido.")
        return documents_for_db


    # Se for cole√ß√£o RAG, processa todos os idiomas dispon√≠veis
    if rag_collection_id:
        allowed_languages = list(article.get("translated_content", {}).keys())
        print(f"üìã Artigo {article_id} (cole√ß√£o RAG) - Todos idiomas permitidos: {allowed_languages}")
    else:
        allowed_languages = get_allowed_languages(article_id, MULTILINGUAL_ARTICLE_IDS)
        print(f"üìã Artigo {article_id} - Idiomas permitidos: {allowed_languages}")

    for lang, content in article.get("translated_content", {}).items():
        # Se for cole√ß√£o RAG, n√£o filtra idiomas
        if not rag_collection_id and lang not in allowed_languages:
            print(f" -> Idioma {lang} pulado para artigo {article_id} (n√£o est√° na lista permitida)")
            continue

        if not (isinstance(content, dict) and content.get("body")):
            continue

        state = content.get("state", "")
        if not (state == "published" or (state == "draft" and rag_collection_id)):
            continue

        print(f"\nüìÑ TESTANDO Artigo ID: {article_id}, Idioma: {lang}, Estado: {state}")
        title = content.get("title", "Sem t√≠tulo")
        print(f"üìù T√≠tulo: {title}")

        # ‚úÖ ETAPA 1: HTML ‚Üí Markdown formatado (preserva estrutura)
        print("\nüîß ETAPA 1: Convertendo HTML ‚Üí Markdown formatado...")
        html_body = content["body"]
        markdown_text = components["text_processor"].process_html_body(html_body)
        
        if not markdown_text:
            print(" -> Artigo pulado pois n√£o cont√©m texto ap√≥s o parsing.")
            continue

        print(f"üìè Markdown gerado: {len(markdown_text)} chars")
        print(f"üî§ Pr√©via do markdown (400 chars):")
        print(f"{markdown_text[:400]}...")
        print("=" * 50)

        # ‚úÖ ETAPA 2: Categoriza√ß√£o (usa markdown formatado)
        print("\nüè∑Ô∏è  ETAPA 2: Categorizando com markdown formatado...")
        category = components["categorizer"].categorize_article(markdown_text, title)
        print(f"üè∑Ô∏è  Categoria identificada: {category}")

        # ‚úÖ ETAPA 3: Chunking sem√¢ntico (usa markdown formatado)
        print("\n‚úÇÔ∏è  ETAPA 3: Chunking sem√¢ntico com markdown...")
        chunks = components["chunker"].chunk_text(markdown_text)
        print(f"‚úÇÔ∏è  Gerados {len(chunks)} chunks sem√¢nticos")
        
        if chunks:
            print(f"\nüìã EXEMPLO DE CHUNK SEM√ÇNTICO (bruto, com markdown):")
            print(f"Tamanho: {len(chunks[0])} chars")
            print(chunks[0][:500] + ("..." if len(chunks[0]) > 500 else ""))
            print("-" * 70)

        # ‚úÖ ETAPA 4: Enriquecimento contextual (usa markdown formatado)
        print("\nüîß ETAPA 4: Enriquecimento contextual...")
        enriched_chunks = components["enricher"].enrich_chunks(chunks, markdown_text)
        print(f"üîß Gerados e enriquecidos {len(enriched_chunks)} chunks finais")

        # Mostra exemplo de chunk enriquecido (ainda com markdown)
        if enriched_chunks:
            print(f"\nüìã EXEMPLO DE CHUNK ENRIQUECIDO (ainda com markdown):")
            print(f"Tamanho: {len(enriched_chunks[0])} chars")
            print(enriched_chunks[0][:600] + ("..." if len(enriched_chunks[0]) > 600 else ""))
            print("-" * 70)

        # ‚úÖ ETAPA 5: Limpeza condicional + Embeddings (APENAS agora limpa)
        print(f"\nüßΩ ETAPA 5: Limpeza condicional + Embeddings...")
        text_cleaner = components["text_cleaner"]
        
        for i, contextualized_chunk in enumerate(enriched_chunks):
            print(f"\nüßΩ Processando chunk {i+1}/{len(enriched_chunks)}...")
            
            # Mostra estado antes da limpeza
            print(f"üì¶ ANTES da limpeza (chunk {i+1}) ‚Äî len={len(contextualized_chunk)}:")
            print(contextualized_chunk[:300] + ("..." if len(contextualized_chunk) > 300 else ""))
            
            # Verifica se precisa de limpeza
            needs_cleaning = text_cleaner.looks_like_markdown_or_html(contextualized_chunk)
            print(f"üîç Precisa de limpeza estrutural? {'‚úÖ SIM' if needs_cleaning else '‚ùå N√ÉO (apenas normaliza√ß√£o)'}")
            
            # Aplica limpeza condicional
            clean_content = text_cleaner.clean_contextual_chunk(contextualized_chunk)
            
            if not clean_content:
                print(f"   ‚ö†Ô∏è Chunk {i+1} vazio ap√≥s limpeza, pulando.")
                continue

            # Mostra resultado da limpeza
            print(f"\nüì¶ DEPOIS da limpeza (chunk {i+1}) ‚Äî len={len(clean_content)}:")
            print(clean_content[:300] + ("..." if len(clean_content) > 300 else ""))

            # Input para embedding
            embedding_input = f"{title}\n\n{clean_content}"
            print(f"\nüß† INPUT para embedding (t√≠tulo + conte√∫do limpo) ‚Äî len={len(embedding_input)}:")
            print(embedding_input[:400] + ("..." if len(embedding_input) > 400 else ""))

            # Gerar embedding
            print(f"\nüß† Gerando embedding para chunk {i+1}...")
            embedding = components["embedding_generator"].generate(embedding_input)
            if not embedding:
                print(f"   ‚ùå Falha ao gerar embedding para chunk {i+1}")
                continue

            print(f"   ‚úÖ Embedding gerado: {len(embedding)} dimens√µes")

            print(f"\nüìã CONTE√öDO FINAL OTIMIZADO (chunk {i+1}):")
            print("=" * 70)
            print(clean_content)
            print("=" * 70)

            # Documento simulado (sem salvar)
            document = {
                "title": title,
                "content": clean_content,
                "category": category,
                "language": lang,
                "embedding": f"[EMBEDDING COM {len(embedding)} DIMENS√ïES]",
                "meta_data": {
                    "source_type": "intercom_article",
                    "article_id": str(article_id),
                    "intercom_url": content.get("url", ""),
                    "article_state": state,
                    "rag_collection_id": rag_collection_id,
                    "is_chunked": True,
                    "chunk_index": i,
                    "total_chunks": len(enriched_chunks),
                    "embedding_model": Config.EMBEDDING_MODEL,
                    "dimensions": Config.EMBEDDING_DIMENSIONS,
                    "cleaned_conditionally": needs_cleaning
                }
            }
            documents_for_db.append(document)

        print(f"\n‚úÖ Artigo {article_id} processado com sucesso!")
        print("=" * 80)

    return documents_for_db


def main():
    """
    Vers√£o de teste para processar apenas 3 artigos seguindo as melhores pr√°ticas.
    Mostra todo o pipeline em a√ß√£o sem salvar no MongoDB.
    """
    try:
        Config.validate()
    except ValueError as e:
        print(f"‚ùå Erro de configura√ß√£o: {e}")
        return

    # ‚úÖ Inicializar componentes incluindo o novo TextCleaner
    components = {
        "intercom_client": IntercomClient(),
        "text_processor": TextProcessor(),           # Agora preserva markdown
        "chunker": LLMChunker(),
        "enricher": ContextualEnricher(),
        "categorizer": ArticleCategorizer(),
        "embedding_generator": EmbeddingGenerator(),
        "text_cleaner": TextCleaner()               # ‚úÖ Novo componente unificado
    }

    print("üß™ MODO TESTE - Processando apenas 3 artigos (SEM salvar no MongoDB)")
    print("üìã Pipeline: HTML ‚Üí Markdown ‚Üí Categorizar ‚Üí Chunking ‚Üí Enriquecimento ‚Üí Limpeza ‚Üí Embeddings")
    print("=" * 80)

    # Configura√ß√£o
    RAG_COLLECTION_ID = None  # Ajuste se quiser filtrar por cole√ß√£o espec√≠fica
    EXCLUDED_ARTICLE_IDS = ["7861154"]  # Exemplo de exclus√£o

    # Busca primeiros 3 artigos
    print("üîç Buscando primeiros 3 artigos...")
    intercom_data = components["intercom_client"].fetch_articles(page_number=1, per_page=3)

    if not intercom_data or "data" not in intercom_data:
        print("‚ö†Ô∏è Nenhum artigo encontrado.")
        return

    print(f"üìä Artigos encontrados para teste: {len(intercom_data['data'])}")

    all_processed_documents = []
    processed_count = 0
    skipped_count = 0

    for i, article in enumerate(intercom_data["data"]):
        print(f"\nüéØ PROCESSANDO ARTIGO {i+1}/3")
        processed_docs = process_single_article_test(
            article, 
            components, 
            RAG_COLLECTION_ID, 
            EXCLUDED_ARTICLE_IDS
        )
        if processed_docs:
            all_processed_documents.extend(processed_docs)
            processed_count += 1
        else:
            skipped_count += 1

    # Relat√≥rio final
    print(f"\nüìà RESUMO DO TESTE")
    print("-" * 80)
    print(f" ‚Ä¢ Artigos processados: {processed_count}")
    print(f" ‚Ä¢ Artigos pulados: {skipped_count}")
    print(f" ‚Ä¢ Total de documentos gerados: {len(all_processed_documents)}")
    print(f" ‚Ä¢ MongoDB: N√ÉO UTILIZADO (modo teste)")

    if all_processed_documents:
        print("\nüìã EXEMPLO DE DOCUMENTO FINAL (campos principais):")
        example_doc = all_processed_documents[0]
        print(f"   T√≠tulo: {example_doc['title']}")
        print(f"   Categoria: {example_doc['category']}")
        print(f"   Idioma: {example_doc['language']}")
        print(f"   Tamanho do conte√∫do: {len(example_doc['content'])} chars")
        print(f"   Pipeline usado: {example_doc['meta_data']['processing_pipeline']}")
        print(f"   Limpeza aplicada: {example_doc['meta_data']['cleaned_conditionally']}")

    print("\nüéâ Teste conclu√≠do! Pipeline seguiu as melhores pr√°ticas:")
    print("   ‚úÖ HTML convertido para markdown formatado")
    print("   ‚úÖ Markdown preservado durante categoriza√ß√£o e chunking")
    print("   ‚úÖ Contextual enrichment trabalhou com markdown")
    print("   ‚úÖ Limpeza condicional aplicada apenas para embeddings")
    print("   ‚úÖ Conte√∫do final otimizado para busca vetorial")


if __name__ == "__main__":
    main()