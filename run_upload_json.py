from config.settings import Config
from src.utils.embeddings import EmbeddingGenerator
from src.mongodb.mongodb_client import MongoDBClient
import json
import re
from pathlib import Path
from typing import Dict, Any, List

class SemanticChunker:
    """Classe para fazer chunking sem√¢ntico inteligente de textos"""
    
    def __init__(self, max_chunk_size: int = 1000, min_chunk_size: int = 100):
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
    
    def chunk_text(self, text: str, title: str = "") -> List[Dict[str, Any]]:
        """
        Faz chunking sem√¢ntico do texto priorizando quebras naturais
        
        Prioridade de quebras:
        1. Linhas duplas ou triplas (\n\n+ ou \n\n\n+)
        2. Pontos seguidos de mai√∫scula
        3. Quebras for√ßadas se muito grande
        """
        chunks = []
        
        if not text.strip():
            return chunks
        
        # Primeiro, tentar quebrar por linhas duplas/triplas
        sections = re.split(r'\n\s*\n+', text)
        
        current_chunk = ""
        chunk_index = 0
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            # Se adicionar esta se√ß√£o n√£o ultrapassar o limite, adiciona
            if len(current_chunk + "\n\n" + section) <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + section
                else:
                    current_chunk = section
            else:
                # Finaliza chunk atual se tem conte√∫do suficiente
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append(self._create_chunk(current_chunk, title, chunk_index))
                    chunk_index += 1
                    current_chunk = section
                else:
                    # Se chunk atual √© muito pequeno, for√ßa jun√ß√£o
                    current_chunk += "\n\n" + section
                
                # Se a se√ß√£o atual √© muito grande, quebra por pontos
                if len(current_chunk) > self.max_chunk_size:
                    sub_chunks = self._split_by_sentences(current_chunk)
                    for i, sub_chunk in enumerate(sub_chunks):
                        if sub_chunk.strip():
                            chunks.append(self._create_chunk(sub_chunk, title, chunk_index))
                            chunk_index += 1
                    current_chunk = ""
        
        # Adicionar √∫ltimo chunk se houver
        if current_chunk.strip() and len(current_chunk) >= self.min_chunk_size:
            chunks.append(self._create_chunk(current_chunk, title, chunk_index))
        
        return chunks
    
    def _split_by_sentences(self, text: str) -> List[str]:
        """Quebra texto por pontos quando necess√°rio"""
        sentences = re.split(r'\.(?=\s+[A-Z])', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # Recolocar o ponto se n√£o terminar com pontua√ß√£o
            if not sentence.endswith(('.', '!', '?', ':')):
                sentence += '.'
            
            if len(current_chunk + " " + sentence) <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _create_chunk(self, content: str, title: str, index: int) -> Dict[str, Any]:
        """Cria estrutura de chunk"""
        return {
            "content": content.strip(),
            "title": title,
            "chunk_index": index,
            "chunk_size": len(content.strip())
        }

def generate_documents_from_json(json_file_path: str) -> List[Dict[str, Any]]:
    """
    Gera documentos a partir do arquivo JSON com chunking sem√¢ntico.
    
    Args:
        json_file_path: Caminho para o arquivo JSON
        
    Returns:
        Lista de documentos processados para inser√ß√£o no MongoDB
    """
    documents = []
    chunker = SemanticChunker(max_chunk_size=1000, min_chunk_size=100)
    
    try:
        # Carregar o arquivo JSON
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Suporta dict com chave 'articles', lista de dicts, ou lista de dicts com chave 'articles'
        if isinstance(data, dict) and "articles" in data:
            articles = data["articles"]
        elif isinstance(data, list):
            # Se for lista de dicts com chave 'articles', pega todos
            if len(data) > 0 and isinstance(data[0], dict) and "articles" in data[0]:
                articles = data[0]["articles"]
            else:
                articles = data
        else:
            articles = []
        print(f"üìä Encontrados {len(articles)} artigos no JSON para processar")

        for i, article in enumerate(articles):
            if not isinstance(article, dict):
                print(f"‚ö†Ô∏è  Pulando artigo {i+1}: formato inv√°lido")
                continue
            if not article.get('title') and not article.get('content'):
                print(f"‚ö†Ô∏è  Pulando artigo {i+1}: sem t√≠tulo nem conte√∫do")
                continue
            title = article.get('title', f'Artigo {i+1}')
            content = article.get('content', '')
            print(f"üìù Processando artigo {i+1}/{len(articles)}: {title[:50]}...")
            # Verificar se o conte√∫do precisa de chunking
            if len(content) <= 1000:
                # Conte√∫do pequeno - criar documento √∫nico
                document = {
                    "title": title,
                    "content": content,
                    "category": article.get("category", ""),
                    "language": article.get("language", "pt-BR"),
                    "meta_data": {
                        "source_type": "json_article",
                        "source_file": Path(json_file_path).name,
                        "article_index": i,
                        "is_chunked": False,
                        "total_chunks": 1,
                        "chunk_index": 0
                    }
                }
                documents.append(document)
            else:
                # Conte√∫do grande - fazer chunking
                chunks = chunker.chunk_text(content, title)
                print(f"üî™ Artigo dividido em {len(chunks)} chunks sem√¢nticos")
                for chunk_data in chunks:
                    document = {
                        "title": title,
                        "content": chunk_data["content"],
                        "category": article.get("category", ""),
                        "language": article.get("language", "pt-BR"),
                        "meta_data": {
                            "source_type": "json_article",
                            "source_file": Path(json_file_path).name,
                            "article_index": i,
                            "is_chunked": True,
                            "chunk_index": chunk_data["chunk_index"],
                            "chunk_size": chunk_data["chunk_size"],
                            "total_chunks": len(chunks)
                        }
                    }
                    documents.append(document)

        print(f"‚úÖ Processados {len(documents)} documentos do JSON")
        return documents
        
    except FileNotFoundError:
        print(f"‚ùå Arquivo JSON n√£o encontrado: {json_file_path}")
        return []
    except json.JSONDecodeError as e:
        print(f"‚ùå Erro ao decodificar JSON: {e}")
        return []
    except Exception as e:
        print(f"‚ùå Erro ao processar JSON: {e}")
        return []

def update_json_knowledge(json_file_path: str):
    """
    Pipeline dedicado a buscar, processar e salvar os documentos
    do arquivo JSON com chunking e embeddings.
    
    Args:
        json_file_path: Caminho para o arquivo JSON
    """
    try:
        Config.validate()
    except ValueError as e:
        print(f"‚ùå Erro de configura√ß√£o: {e}")
        return

    # Inicializar os componentes necess√°rios
    embedding_generator = EmbeddingGenerator()
    mongodb_client = MongoDBClient()

    print("üöÄ Iniciando pipeline de atualiza√ß√£o de conhecimento JSON...")
    print(f"üìÑ Arquivo: {json_file_path}")
    
    # ETAPA 1: Gerar documentos a partir do JSON
    json_documents = generate_documents_from_json(json_file_path)
    
    if not json_documents:
        print("‚ùå Nenhum documento foi gerado do JSON.")
        return
    
    # ETAPA 2: Gerar embeddings para cada documento
    print(f"\nü§ñ Gerando embeddings para {len(json_documents)} documentos...")
    processed_documents = []
    
    for i, doc in enumerate(json_documents):
        print(f"üîÑ Gerando embedding {i+1}/{len(json_documents)}")
        
        # Texto para embedding (t√≠tulo + conte√∫do)
        embedding_text = f"{doc['title']}. {doc['content']}"
        embedding = embedding_generator.generate(embedding_text)
        
        if embedding:
            doc['embedding'] = embedding
            doc['meta_data']['embedding_model'] = Config.EMBEDDING_MODEL
            doc['meta_data']['dimensions'] = Config.EMBEDDING_DIMENSIONS
            processed_documents.append(doc)
        else:
            print(f"‚ùå Erro ao gerar embedding para documento {i+1}")
    
    # ETAPA 3: Salvar os documentos no MongoDB
    if processed_documents:
        print(f"\nüíæ Salvando {len(processed_documents)} documentos no MongoDB...")
        mongodb_client.upsert_documents(processed_documents)
        
        # Estat√≠sticas
        chunked_docs = len([d for d in processed_documents if d["meta_data"]["is_chunked"]])
        single_docs = len([d for d in processed_documents if not d["meta_data"]["is_chunked"]])
        
        print(f"üìä Estat√≠sticas:")
        print(f"   - Documentos √∫nicos: {single_docs}")
        print(f"   - Chunks de documentos grandes: {chunked_docs}")
        print(f"   - Total processado: {len(processed_documents)}")
    else:
        print("‚ùå Nenhum documento foi processado com sucesso.")

    print("\nüéâ Pipeline de conhecimento JSON conclu√≠do!")

def main():
    """
    Fun√ß√£o principal que pode ser usada para testar o pipeline
    """
    import sys
    
    if len(sys.argv) < 2:
        print("‚ùå Uso: python run_json_pipeline.py <caminho_para_arquivo_json>")
        print("üìÑ Exemplo: python run_json_pipeline.py data/artigos.json")
        return
    
    json_file_path = sys.argv[1]
    update_json_knowledge(json_file_path)

if __name__ == "__main__":
    main()